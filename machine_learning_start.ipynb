{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We're gonna start doing some machine learning with spark \n",
    "# ----------- Linear regression -------------------\n",
    "# we're gonna predict miles per gallon based on weight using linear regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "auto-miles-per-gallon.csv MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoData = sc.textFile('auto-miles-per-gallon.csv')\n",
    "autoData.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MPG,CYLINDERS,DISPLACEMENT,HORSEPOWER,WEIGHT,ACCELERATION,MODELYEAR,NAME',\n",
       " '18,8,307,130,3504,12,70,chevrolet chevelle malibu',\n",
       " '15,8,350,165,3693,11.5,70,buick skylark 320',\n",
       " '18,8,318,150,3436,11,70,plymouth satellite',\n",
       " '16,8,304,150,3433,12,70,amc rebel sst',\n",
       " '17,8,302,140,3449,10.5,70,ford torino',\n",
       " '15,8,429,198,4341,10,70,ford galaxie 500',\n",
       " '14,8,454,220,4354,9,70,chevrolet impala',\n",
       " '14,8,440,215,4312,8.5,70,plymouth fury iii',\n",
       " '14,8,455,225,4425,10,70,pontiac catalina',\n",
       " '15,8,390,190,3850,8.5,70,amc ambassador dpl',\n",
       " '15,8,383,170,3563,10,70,dodge challenger se',\n",
       " \"14,8,340,160,3609,8,70,plymouth 'cuda 340\",\n",
       " '15,8,400,150,3761,9.5,70,chevrolet monte carlo',\n",
       " '14,8,455,225,3086,10,70,buick estate wagon (sw)',\n",
       " '24,4,113,95,2372,15,70,toyota corona mark ii',\n",
       " '22,6,198,95,2833,15.5,70,plymouth duster',\n",
       " '18,6,199,97,2774,15.5,70,amc hornet',\n",
       " '21,6,200,85,2587,16,70,ford maverick',\n",
       " '27,4,97,88,2130,14.5,70,datsun pl510']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's now take a quick look at the data\n",
    "autoData.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's now remove the first line\n",
    "\n",
    "dataLines = autoData.filter(lambda x: 'MPG' not in x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to now turn our RDD into a 'dense vector'. In order to do that, we need to:\n",
    "1. Remove unwanted columns\n",
    "2. Change non-numeric values to numeric.\n",
    "\n",
    "In order to do that, we will now create the `transformToNumeric`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgHP = sc.broadcast(80.0)\n",
    "\n",
    "def transformToNumeric(Str):\n",
    "    #global avgHP\n",
    "    att = Str.split(',')\n",
    "    hp = att[3]\n",
    "    if hp == '?':\n",
    "        hp = 80.0\n",
    "    #we now filter the unwanted columns\n",
    "    values = Vectors.dense([float(att[0]), float(att[1]), hp, float(att[5]), float(att[6])])\n",
    "    return values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([18.0, 8.0, 130.0, 12.0, 70.0]),\n",
       " DenseVector([15.0, 8.0, 165.0, 11.5, 70.0]),\n",
       " DenseVector([18.0, 8.0, 150.0, 11.0, 70.0]),\n",
       " DenseVector([16.0, 8.0, 150.0, 12.0, 70.0]),\n",
       " DenseVector([17.0, 8.0, 140.0, 10.5, 70.0]),\n",
       " DenseVector([15.0, 8.0, 198.0, 10.0, 70.0]),\n",
       " DenseVector([14.0, 8.0, 220.0, 9.0, 70.0]),\n",
       " DenseVector([14.0, 8.0, 215.0, 8.5, 70.0]),\n",
       " DenseVector([14.0, 8.0, 225.0, 10.0, 70.0]),\n",
       " DenseVector([15.0, 8.0, 190.0, 8.5, 70.0]),\n",
       " DenseVector([15.0, 8.0, 170.0, 10.0, 70.0]),\n",
       " DenseVector([14.0, 8.0, 160.0, 8.0, 70.0]),\n",
       " DenseVector([15.0, 8.0, 150.0, 9.5, 70.0]),\n",
       " DenseVector([14.0, 8.0, 225.0, 10.0, 70.0]),\n",
       " DenseVector([24.0, 4.0, 95.0, 15.0, 70.0]),\n",
       " DenseVector([22.0, 6.0, 95.0, 15.5, 70.0]),\n",
       " DenseVector([18.0, 6.0, 97.0, 15.5, 70.0]),\n",
       " DenseVector([21.0, 6.0, 85.0, 16.0, 70.0]),\n",
       " DenseVector([27.0, 4.0, 88.0, 14.5, 70.0]),\n",
       " DenseVector([26.0, 4.0, 46.0, 20.5, 70.0]),\n",
       " DenseVector([25.0, 4.0, 87.0, 17.5, 70.0]),\n",
       " DenseVector([24.0, 4.0, 90.0, 14.5, 70.0]),\n",
       " DenseVector([25.0, 4.0, 95.0, 17.5, 70.0]),\n",
       " DenseVector([26.0, 4.0, 113.0, 12.5, 70.0]),\n",
       " DenseVector([21.0, 6.0, 90.0, 15.0, 70.0]),\n",
       " DenseVector([10.0, 8.0, 215.0, 14.0, 70.0]),\n",
       " DenseVector([10.0, 8.0, 200.0, 15.0, 70.0]),\n",
       " DenseVector([11.0, 8.0, 210.0, 13.5, 70.0]),\n",
       " DenseVector([9.0, 8.0, 193.0, 18.5, 70.0])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoVectors = dataLines.map(transformToNumeric)\n",
    "autoVectors.take(29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now do Statistical Analysis over the Data we have collected.\n",
    "\n",
    "Remember that our columns are, respectively **MPG, CYLINDERS, HP, ACCELERATION, MODEL YEAR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "autoStats = Statistics.colStats(autoVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 23.51457286   5.45477387 104.10050251  15.56809045  76.01005025] [  61.08961077    2.89341544 1468.09062947    7.60484823   13.67244282] [ 9.  3. 46.  8. 70.] [ 46.6   8.  230.   24.8  82. ]\n"
     ]
    }
   ],
   "source": [
    "#This are some of the operations we can do with this module\n",
    "print(\n",
    "autoStats.mean(),\n",
    "autoStats.variance(),\n",
    "autoStats.min(),\n",
    "autoStats.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.77539629, -0.77463084,  0.42028891,  0.57926713],\n",
       "       [-0.77539629,  1.        ,  0.84275215, -0.50541949, -0.3487458 ],\n",
       "       [-0.77463084,  0.84275215,  1.        , -0.68829885, -0.41559383],\n",
       "       [ 0.42028891, -0.50541949, -0.68829885,  1.        ,  0.28813695],\n",
       "       [ 0.57926713, -0.3487458 , -0.41559383,  0.28813695,  1.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#with the following commnand, we can obtain a correlation matrix among the different attributes. In other words, we can see how \n",
    "# correlated these are among each other\n",
    "\n",
    "Statistics.corr(autoVectors)\n",
    "\n",
    "#so for instance we can see that there's a strong correlation between the values of HP and Cylinders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we're gonna transform our data into a Labeled Point. Let's remember that a Labeled Point is a vector that contains first the value (outcome) and the rest are all the attributes that we use to predict our value - > (y, x_1, x_2,..., x_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we create the function for our Labeled Point\n",
    "# the first value will be the MPG, and the rest are the other attributes\n",
    "#note that we decided to drop the value for the MODEL YEAR as it doesn't have a strong correlation (as we can see from the corr matrix)\n",
    "#actually, in the video they got rid of ACCELERATION instead, but it doesn't really matter.\n",
    "\n",
    "def transformToLabeledPoint(Str):\n",
    "    lp = (float(Str[0]), Vectors.dense(Str[1], Str[2], Str[3]))\n",
    "    return lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoLp = autoVectors.map(transformToLabeledPoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And next we're gonna use the `SQLContext` that we imported a couple of cells ago to create a DataFrame (as we've already learned in past lectures), which is what we're gonna use next for our modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------+\n",
      "|label|        features|\n",
      "+-----+----------------+\n",
      "| 18.0|[8.0,130.0,12.0]|\n",
      "| 15.0|[8.0,165.0,11.5]|\n",
      "| 18.0|[8.0,150.0,11.0]|\n",
      "| 16.0|[8.0,150.0,12.0]|\n",
      "| 17.0|[8.0,140.0,10.5]|\n",
      "| 15.0|[8.0,198.0,10.0]|\n",
      "| 14.0| [8.0,220.0,9.0]|\n",
      "| 14.0| [8.0,215.0,8.5]|\n",
      "| 14.0|[8.0,225.0,10.0]|\n",
      "| 15.0| [8.0,190.0,8.5]|\n",
      "+-----+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "autoDf = sqlContext.createDataFrame(autoLp, ['label', 'features'])\n",
    "autoDf.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to find **correlations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t-0.775396\n",
      "1\t-0.774631\n",
      "2\t0.420289\n"
     ]
    }
   ],
   "source": [
    "#we first find the number of features in each row (which we know to be 3)\n",
    "numFeatures = autoDf.take(1)[0].features.size\n",
    "labelRDD = autoDf.rdd.map(lambda x: float(x.label))\n",
    "for i in range(numFeatures):\n",
    "    featureRDD = autoDf.rdd.map(lambda x: x.features[i])\n",
    "    corr = Statistics.corr(labelRDD, featureRDD, 'pearson')\n",
    "    print('%d\\t%g' % (i, corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, we obtain the same result in the previous correlation numbers, as we did with the correlation matrix beforehand (as it's doing the same operations).\n",
    "\n",
    "Now, we want to do some actual Machine Learning. In order to do that, we're gonna split our data into **Training Data** and **Test Data**, as we know already. We're gonna do respectively 90% and 10%, because our sample is small, so we want to use as much training data as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347 51\n"
     ]
    }
   ],
   "source": [
    "(trainingData, testData) = autoDf.randomSplit([0.9,0.1])\n",
    "print(\n",
    "trainingData.count(),\n",
    "testData.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now build the Model on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------+\n",
      "|label|        features|\n",
      "+-----+----------------+\n",
      "| 10.0|[8.0,200.0,15.0]|\n",
      "| 10.0|[8.0,215.0,14.0]|\n",
      "| 11.0|[8.0,150.0,14.0]|\n",
      "| 11.0|[8.0,208.0,11.0]|\n",
      "| 11.0|[8.0,210.0,13.5]|\n",
      "| 12.0|[8.0,160.0,13.5]|\n",
      "| 12.0|[8.0,167.0,12.5]|\n",
      "| 12.0|[8.0,180.0,12.5]|\n",
      "| 12.0|[8.0,198.0,11.5]|\n",
      "| 13.0|[8.0,129.0,12.0]|\n",
      "| 13.0|[8.0,130.0,14.0]|\n",
      "| 13.0|[8.0,140.0,16.0]|\n",
      "| 13.0|[8.0,145.0,13.0]|\n",
      "| 13.0|[8.0,150.0,12.0]|\n",
      "| 13.0|[8.0,150.0,14.5]|\n",
      "| 13.0|[8.0,155.0,13.5]|\n",
      "| 13.0|[8.0,158.0,13.0]|\n",
      "| 13.0|[8.0,165.0,12.0]|\n",
      "| 13.0|[8.0,170.0,12.0]|\n",
      "| 13.0|[8.0,175.0,12.0]|\n",
      "+-----+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingData.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Column features must be of type struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually struct<type:tinyint,size:int,indices:array<int>,values:array<double>>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-a1681b9dd668>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaxIter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeaturesCol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'features'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabelCol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'label'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlrModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\adomingu\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    127\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32mc:\\users\\adomingu\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\adomingu\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    316\u001b[0m         \"\"\"\n\u001b[0;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\adomingu\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\adomingu\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    135\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m                 \u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\adomingu\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: requirement failed: Column features must be of type struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually struct<type:tinyint,size:int,indices:array<int>,values:array<double>>."
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(maxIter = 10, featuresCol = 'features', labelCol = 'label') \n",
    "lrModel = lr.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
